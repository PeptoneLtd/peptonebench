{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c7b712",
   "metadata": {},
   "source": [
    "# Create PeptoneDB-SAXS\n",
    "This notebook will download current data from [SASBDB](https://www.sasbdb.org) and filter entries accoding to various criteria to obtain an updated version of the PeptoneDB-SAXS dataset.\n",
    "\n",
    "Requires some third party software:\n",
    "- BIFT, https://github.com/ehb54/GenApp-BayesApp\n",
    "- MMseqs2, https://github.com/soedinglab/MMseqs2\n",
    "- ADOPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import subprocess\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peptonebench import saxs\n",
    "\n",
    "DATA_PATH = os.path.abspath(\"../datasets/PeptoneDB-SAXS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{DATA_PATH}/sasbdb-data\", exist_ok=True)\n",
    "if not os.path.exists(f\"{DATA_PATH}/sasbdb-data/all_sasbdb_proteins.json\"):\n",
    "    response = requests.get(\"https://www.sasbdb.org/rest-api/entry/codes/molecular_type/protein/\")\n",
    "    with open(f\"{DATA_PATH}/sasbdb-data/all_sasbdb_proteins.json\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "with open(f\"{DATA_PATH}/sasbdb-data/all_sasbdb_proteins.json\") as file:\n",
    "    json_data = json.load(file)\n",
    "print(f\"tot SASBDB entries: {len(json_data):_}\")\n",
    "\n",
    "for entry in tqdm(json_data):\n",
    "    label = entry[\"code\"]\n",
    "    filename = f\"{DATA_PATH}/sasbdb-data/{label}.json\"\n",
    "    if not os.path.exists(filename) or os.path.getsize(filename) == 0:\n",
    "        response = requests.get(f\"https://www.sasbdb.org/rest-api/entry/summary/{label}/\")\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586589ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter for monomeric proteins with standard amino acids\n",
    "standard_amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "sequences = {}\n",
    "pHs = {}\n",
    "for file in tqdm(sorted(glob(f\"{DATA_PATH}/sasbdb-data/SASD*.json\"))):\n",
    "    try:\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "        molecule_data = data[\"experiment\"][\"sample\"][\"molecule\"]\n",
    "        if (\n",
    "            len(molecule_data) == 1\n",
    "            and molecule_data[0][\"molecular_type\"] == \"protein\"\n",
    "            and molecule_data[0][\"oligomerization\"] == \"monomer\"\n",
    "        ):\n",
    "            fasta = molecule_data[0][\"sequence\"]\n",
    "            if isinstance(fasta, str) and fasta != \"NA\":\n",
    "                if fasta.startswith(\">\"):\n",
    "                    fasta = fasta[fasta.index(\"\\n\") :]\n",
    "                fasta = fasta.replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\").upper()\n",
    "                if (\n",
    "                    \"project\" in data\n",
    "                    and \"publication\" in data[\"project\"]\n",
    "                    and data[\"project\"][\"publication\"] is not None\n",
    "                    and \"doi\" in data[\"project\"][\"publication\"]\n",
    "                    and data[\"project\"][\"publication\"][\"doi\"] == \"10.1073/pnas.1704692114\"\n",
    "                ):  # in this publication they are using a non-standard amino acid U=p-acetylphenylalanine\n",
    "                    fasta = fasta.replace(\"U\", \"F\")\n",
    "                invalid_aa = [aa for aa in fasta if aa not in standard_amino_acids]\n",
    "                if len(invalid_aa) > 0:\n",
    "                    print(f\"--- skipping {data['code']} --- invalid amino acid: {invalid_aa}\")\n",
    "                else:\n",
    "                    sequences[data[\"code\"]] = fasta\n",
    "                    pHs[data[\"code\"]] = data[\"experiment\"][\"sample\"][\"buffer\"][\"ph\"]\n",
    "                    if pHs[data[\"code\"]] is None:\n",
    "                        pHs[data[\"code\"]] = np.nan\n",
    "    except Exception as e:\n",
    "        print(f\" --- skipping {os.path.basename(file)} --- {e}\")\n",
    "\n",
    "print(f\"\\nkept {len(sequences):_} sequences out of {len(json_data):_} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e87c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter for length\n",
    "current_labels = list(sequences.keys())\n",
    "\n",
    "min_length = 15\n",
    "max_length = 500  # same as CS dataset. It seems that there are no disordered proteins longer than this\n",
    "\n",
    "print(f\"entries with length < {min_length}: \", np.sum([len(sequences[label]) < min_length for label in current_labels]))\n",
    "print(f\"entries with length > {max_length}: \", np.sum([len(sequences[label]) > max_length for label in current_labels]))\n",
    "plt.hist([len(sequences[label]) for label in current_labels], bins=\"auto\")\n",
    "plt.axvspan(min_length, max_length, color=\"red\", alpha=0.2, label=\"length filter\")\n",
    "plt.xlabel(\"sequence length\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "length_filtered_labels = [label for label in current_labels if min_length <= len(sequences[label]) <= max_length]\n",
    "print(f\"\\nkept {len(length_filtered_labels):_} sequences out of {len(current_labels):_} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter pH\n",
    "current_labels = length_filtered_labels\n",
    "\n",
    "pH_range = [4.0, 10.0]  # same as trizod moderate\n",
    "pH_filtered_labels = [\n",
    "    label for label in current_labels if not np.isnan(pHs[label]) and pH_range[0] < pHs[label] < pH_range[1]\n",
    "]\n",
    "tot_none_pH = np.sum(np.isnan([pHs[label] for label in current_labels]))\n",
    "print(\"tot None pH:\", tot_none_pH)\n",
    "print(\n",
    "    \"tot pH out of range:\",\n",
    "    len(current_labels) - len(pH_filtered_labels) - tot_none_pH,\n",
    ")\n",
    "\n",
    "plt.hist([pHs[label] for label in current_labels], bins=\"auto\")\n",
    "plt.axvspan(6.0, 8.0, color=\"red\", alpha=0.1)\n",
    "plt.axvspan(4.0, 10.0, color=\"red\", alpha=0.1)\n",
    "plt.axvline(pH_range[0], color=\"red\", linestyle=\":\")\n",
    "plt.axvline(pH_range[1], color=\"red\", linestyle=\":\")\n",
    "plt.xlabel(\"pH\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"selected {len(pH_filtered_labels):_} sequences out of {len(current_labels):_} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68efc6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_labels = pH_filtered_labels\n",
    "\n",
    "preprocessed_labels = []\n",
    "for label in tqdm(current_labels):\n",
    "    with open(f\"{DATA_PATH}/sasbdb-data/{label}.json\") as f:\n",
    "        data = json.load(f)\n",
    "    if not os.path.exists(f\"{DATA_PATH}/sasbdb-data/{label}.out\"):\n",
    "        try:\n",
    "            subprocess.run([\"wget\", data[\"pddf_data\"], \"-q\", \"-O\", f\"{DATA_PATH}/sasbdb-data/{label}.out\"], check=True)\n",
    "        except TypeError as e:\n",
    "            print(f\"--- skipping {label}: out file not available ({e})\")\n",
    "            continue\n",
    "    try:\n",
    "        parsed_data = saxs.parse_sasbdb_out(f\"{DATA_PATH}/sasbdb-data/{label}.out\", rescale_to_dat=False)\n",
    "    except Exception as e:\n",
    "        print(f\"--- skipping {label}: out ({e})\")\n",
    "        continue\n",
    "    if len(parsed_data) == 0:\n",
    "        print(f\"--- skipping {label}: out has no data\")\n",
    "    else:\n",
    "        if not os.path.exists(f\"{DATA_PATH}/sasbdb-data/{label}.dat\"):\n",
    "            subprocess.run(\n",
    "                [\"wget\", data[\"intensities_data\"], \"-q\", \"-O\", f\"{DATA_PATH}/sasbdb-data/{label}.dat\"], check=True\n",
    "            )\n",
    "        try:\n",
    "            parsed_data = saxs.parse_sasbdb_dat(f\"{DATA_PATH}/sasbdb-data/{label}.dat\")\n",
    "        except Exception as e:\n",
    "            print(f\"--- skipping {label}: dat --- {e}\")\n",
    "            continue\n",
    "        if len(parsed_data) == 0:\n",
    "            print(f\"+++ WARNING: {label}: dat has no data, skipping\")\n",
    "        else:\n",
    "            preprocessed_labels.append(label)\n",
    "print(f\"\\nkept {len(preprocessed_labels):_} preprocessed labels out of {len(current_labels):_} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d45bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply BIFT and get scale factors\n",
    "current_labels = preprocessed_labels\n",
    "\n",
    "PROCESSING_DIR = f\"{DATA_PATH}/sasbdb-processing/bift_processing\"\n",
    "os.makedirs(PROCESSING_DIR, exist_ok=True)\n",
    "UNITS_SCALING = {\"1/A\": 1, \"1/nm\": 0.1}  # as Pepsi-SAXS, setting units to 1/A\n",
    "\n",
    "BIFT_EXEC = f\"{DATA_PATH}/sasbdb-processing/bift\"\n",
    "if not os.path.exists(BIFT_EXEC):\n",
    "    print(\"BIFT executable not found. Trying to get it and install it...\")\n",
    "    subprocess.run(\n",
    "        [\"wget\", \"https://raw.githubusercontent.com/ehb54/GenApp-BayesApp/main/bin/source/bift.f\"], check=True\n",
    "    )\n",
    "    subprocess.run([\"gfortran\", \"bift.f\", \"-march=native\", \"-O2\", \"-o\", \"bift\"], check=True)\n",
    "    subprocess.run([\"mv\", \"bift\", BIFT_EXEC], check=True)\n",
    "    subprocess.run([\"rm\", \"bift.f\"], check=True)\n",
    "\n",
    "\n",
    "def process_file(label: str) -> tuple[str, pd.DataFrame, float] | None:\n",
    "    filename = f\"{DATA_PATH}/sasbdb-data/{label}.out\"\n",
    "    with open(f\"{DATA_PATH}/sasbdb-data/{label}.json\") as f:\n",
    "        angular_unit = json.load(f)[\"angular_unit\"]\n",
    "\n",
    "    tmp_dir = os.path.join(PROCESSING_DIR, label)\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "    filtered_data = saxs.parse_sasbdb_out(filename)\n",
    "    if len(filtered_data) == 0:\n",
    "        print(f\"skipping {label}: no filtered data found\")\n",
    "        return None\n",
    "    filtered_data.loc[filtered_data[\"sigma\"] == -1, \"sigma\"] = np.nan\n",
    "    if filtered_data.isnull().values.any():\n",
    "        print(f\"Interpolating missing values ({filtered_data.isnull().sum() / len(filtered_data):.2%}) in {label}\")\n",
    "        filtered_data = filtered_data.interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "    data = saxs.parse_sasbdb_dat(filename.replace(\".out\", \".dat\"))\n",
    "    if len(data) == 0:\n",
    "        print(f\"+++ WARNING: no raw data found for {label}, using filtered data only\")\n",
    "        normalization_factor = 1.0\n",
    "    else:\n",
    "        ## sometimes the filtered data was also changed from 1/nm to 1/A, we must rescale it back.\n",
    "        ## if filtered_data was a perfect subset of data, then checking min and max q values would be enough\n",
    "        ## but somehow that's not the case, so we need to also estimate the scaling factor\n",
    "        scaling = np.diff(data[\"q\"]).mean() / np.diff(filtered_data[\"q\"]).mean()\n",
    "        if scaling > 1.5 and (filtered_data[\"q\"].min() < data[\"q\"].min() or filtered_data[\"q\"].max() > data[\"q\"].max()):\n",
    "            scaling = 10 if scaling < 90 else 100  # SASDBL3 and SASDBN3 were scaled by 100!\n",
    "            print(f\"--- {label}: rescaling '.out' q data back to '.dat' units ({scaling}x)\")\n",
    "            filtered_data[\"q\"] *= scaling\n",
    "        if data[\"q\"].min() <= filtered_data[\"q\"].min():\n",
    "            fdt_i = 0\n",
    "            dt_i = (data[\"q\"] - filtered_data[\"q\"].iloc[0]).abs().idxmin()\n",
    "        else:  # this actually happens quite often\n",
    "            dt_i = 0\n",
    "            fdt_i = (filtered_data[\"q\"] - data[\"q\"].iloc[0]).abs().idxmin()\n",
    "        ## the following is equivalent to Svergun et al. (1995), but can be done when number of q is different\n",
    "        normalization_factor = filtered_data[\"I(q)\"].iloc[fdt_i] / data[\"I(q)\"].iloc[dt_i]\n",
    "    filtered_data[\"sigma\"] /= normalization_factor\n",
    "    filtered_data[\"I(q)\"] /= normalization_factor\n",
    "    filtered_data[\"q\"] *= UNITS_SCALING[angular_unit]\n",
    "    if filtered_data[\"q\"].max() > 2.0 and UNITS_SCALING[\"1/A\"] == 1:\n",
    "        print(f\"--- {label}: this is likely in 1/nm, converting to 1/A. q_max = {filtered_data['q'].max()} * 0.1\")\n",
    "        filtered_data[\"q\"] *= UNITS_SCALING[\"1/nm\"]\n",
    "    try:\n",
    "        if not os.path.exists(f\"{tmp_dir}/scale_factor.dat\"):\n",
    "            filtered_data.to_csv(f\"{tmp_dir}/experimental.dat\", index=False, sep=\"\\t\", header=False)\n",
    "            with open(f\"{tmp_dir}/inputfile.dat\", \"w\") as f:\n",
    "                f.write(\"experimental.dat\" + 17 * \"\\n\")\n",
    "            subprocess.run(f\"cd {tmp_dir} && {BIFT_EXEC} > bift.log 2>&1\", shell=True, check=True)\n",
    "        scale_factor = np.loadtxt(f\"{tmp_dir}/scale_factor.dat\")\n",
    "        assert np.all(scale_factor[:, 1] == scale_factor[0, 1]), \"scale factors should be equal\"\n",
    "        assert np.allclose(filtered_data[\"q\"], scale_factor[:, 0]), \"mismatched q values\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {label}: {e}\")\n",
    "        return None\n",
    "    if scale_factor[0, 1] > 100:\n",
    "        print(f\"--- skipping {label}: unreasonably large scale factor, {scale_factor[0, 1]}\")\n",
    "        return None\n",
    "    filtered_data[\"sigma\"] *= scale_factor[0, 1]\n",
    "\n",
    "    return label, filtered_data, scale_factor[0, 1]\n",
    "\n",
    "\n",
    "all_bift_data = Parallel(n_jobs=64)(delayed(process_file)(filename) for filename in tqdm(current_labels))\n",
    "preprocessed_bift_labels = sorted([data[0] for data in all_bift_data if data is not None])\n",
    "saxs_curves = {data[0]: data[1] for data in all_bift_data if data is not None}\n",
    "scale_factors = {data[0]: data[2] for data in all_bift_data if data is not None}\n",
    "\n",
    "plt.hist([np.log10(data[2]) for data in all_bift_data if data is not None], bins=20)\n",
    "plt.xlabel(\"log10 BIFT scale factors\")\n",
    "plt.show()\n",
    "print(f\"\\nkept {len(preprocessed_bift_labels):_} labels out of {len(current_labels):_} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20324a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster with mmseqs2 and get gscores with ADOPT2. This is not an easily portable step, sorry\n",
    "current_labels = preprocessed_bift_labels\n",
    "\n",
    "if not os.path.exists(f\"{DATA_PATH}/sasbdb-processing/clustered_rep_seq.fasta\"):\n",
    "    print(\"writing current sequences to DB.fasta\")\n",
    "    with open(f\"{DATA_PATH}/sasbdb-processing/DB.fasta\", \"w\") as f:\n",
    "        for label in current_labels:\n",
    "            f.write(f\">{label}\\n{sequences[label]}\\n\")\n",
    "    print(\"clustering with mmseqs2...\")\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"mmseqs_avx2\",\n",
    "            \"easy-cluster\",\n",
    "            f\"{DATA_PATH}/sasbdb-processing/DB.fasta\",\n",
    "            f\"{DATA_PATH}/sasbdb-processing/clustered\",\n",
    "            \"/tmp\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"clustered_rep_seq.fasta already exists, skipping clustering step\")\n",
    "\n",
    "with open(f\"{DATA_PATH}/sasbdb-processing/clustered_rep_seq.fasta\") as f:\n",
    "    all_clustered_fasta = f.read()\n",
    "clustered_labels = sorted([entry[:7] for entry in all_clustered_fasta.split(\">\")[1:]])\n",
    "\n",
    "adopt_filename = f\"{DATA_PATH}/sasbdb-processing/gscores_adopt2.csv\"\n",
    "if not os.path.exists(adopt_filename):\n",
    "    print(\"calculating gscores with ADOPT2...\")\n",
    "    from oppenheimer import adopt2\n",
    "\n",
    "    adopt2_res = adopt2.get_scores(all_clustered_fasta)\n",
    "    adopt2_scores = {entry[\"label\"]: entry[\"g_scores\"] for entry in adopt2_res}\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"label\": list(adopt2_scores.keys()),\n",
    "            \"mean_gscore_adopt2\": [np.nanmean(sc) for sc in adopt2_scores.values()],\n",
    "            \"gscores_adopt2\": [json.dumps(sc) for sc in adopt2_scores.values()],\n",
    "        },\n",
    "    ).to_csv(adopt_filename, index=False)\n",
    "gscores_adopt2_df = pd.read_csv(\n",
    "    adopt_filename, index_col=\"label\", converters={\"gscores_adopt2\": lambda s: np.asarray(json.loads(s), dtype=float)}\n",
    ")\n",
    "assert all(label in gscores_adopt2_df.index for label in clustered_labels), \"missing labels in gscores_adopt2_df\"\n",
    "assert gscores_adopt2_df[\"gscores_adopt2\"].apply(np.nanmin).min() >= 0, \"negative G-scores found\"\n",
    "assert gscores_adopt2_df[\"gscores_adopt2\"].apply(np.nanmax).max() <= 1, \"G-scores above 1 found\"\n",
    "\n",
    "disorder_threshold = 0.5\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.suptitle(f\"ADOPT2 G-scores distribution, {len(gscores_adopt2_df):_} sequences\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(gscores_adopt2_df[\"mean_gscore_adopt2\"], bins=\"auto\")\n",
    "plt.axvline(\n",
    "    disorder_threshold,\n",
    "    color=\"k\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"tot ordered = {np.sum(gscores_adopt2_df['mean_gscore_adopt2'] < disorder_threshold):_}\"\n",
    "    f\"\\ntot disordered = {np.sum(gscores_adopt2_df['mean_gscore_adopt2'] >= disorder_threshold):_}\",\n",
    ")\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel(\"ADOPT2 mean G-score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    [len(sequences[label]) for label in clustered_labels],\n",
    "    gscores_adopt2_df.loc[clustered_labels, \"mean_gscore_adopt2\"],\n",
    "    c=gscores_adopt2_df.loc[clustered_labels, \"mean_gscore_adopt2\"],\n",
    "    s=1,\n",
    "    cmap=\"seismic\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "plt.ylim(0, 1)\n",
    "plt.axhline(disorder_threshold, color=\"k\", linestyle=\":\")\n",
    "plt.xlabel(\"sequence length\")\n",
    "plt.ylabel(\"ADOPT2 mean G-score\")\n",
    "plt.colorbar(label=\"ADOPT2 mean G-score\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"\\nkept {len(clustered_labels):_} clustered sequences out of {len(current_labels):_} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7603e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_labels = sorted(clustered_labels)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"label\": current_labels,\n",
    "        \"sequence\": [sequences[label] for label in current_labels],\n",
    "        \"length\": [len(sequences[l]) for l in current_labels],\n",
    "        \"pH\": [pHs[label] for label in current_labels],\n",
    "        \"mean_gscore_adopt2\": gscores_adopt2_df.loc[current_labels, \"mean_gscore_adopt2\"],\n",
    "        \"gscores_adopt2\": [\n",
    "            json.dumps(list(gscores_adopt2_df.loc[label, \"gscores_adopt2\"])) for label in current_labels\n",
    "        ],\n",
    "    },\n",
    ").to_csv(f\"{DATA_PATH}/dataset.csv\", index=False)\n",
    "\n",
    "os.makedirs(f\"{DATA_PATH}/sasbdb-clean_data\", exist_ok=True)\n",
    "for label in current_labels:\n",
    "    with open(f\"{DATA_PATH}/sasbdb-clean_data/{label}-bift.dat\", \"w\") as f:\n",
    "        f.write(f\"# q I(q) sigma # bift_factor={scale_factors[label]}\\n\")\n",
    "        for _, row in saxs_curves[label].iterrows():\n",
    "            f.write(f\"{row['q']}\\t{row['I(q)']}\\t{row['sigma']}\\n\")\n",
    "\n",
    "print(f\"results saved, {len(current_labels):_} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the dataset is small enough that we don't need to rebalance it.\n",
    "## also, depending on the disorder predictor used, the rebalancing would be different.\n",
    "N_BINS_TOTAL = 31\n",
    "np.random.seed(42)\n",
    "\n",
    "current_labels = sorted(clustered_labels)\n",
    "data = gscores_adopt2_df[\"mean_gscore_adopt2\"].loc[current_labels].to_numpy()\n",
    "\n",
    "hist, bins_edges = np.histogram(data, bins=N_BINS_TOTAL)\n",
    "bins_centers = (bins_edges[:-1] + bins_edges[1:]) / 2\n",
    "threshold = 0.5\n",
    "sigmoid = np.minimum(1, 1.3 / (1 + np.exp(-8.5 * (bins_centers - threshold))))\n",
    "scaled_hist = np.copy(hist) * sigmoid\n",
    "\n",
    "plt.plot(\n",
    "    bins_centers,\n",
    "    hist / hist.max(),\n",
    "    \"-+\",\n",
    "    label=f\"original\\nratio: {hist[bins_centers > threshold].sum() / hist[bins_centers < threshold].sum():.2f}\",\n",
    ")\n",
    "plt.plot(\n",
    "    bins_centers,\n",
    "    scaled_hist / scaled_hist.max(),\n",
    "    \"-x\",\n",
    "    label=f\"rescaled\\nratio: {scaled_hist[bins_centers > threshold].sum() / scaled_hist[bins_centers < threshold].sum():.2f}\",\n",
    ")\n",
    "plt.plot(bins_centers, sigmoid, label=\"sigmoid\")\n",
    "# plt.xlim(0, 1)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Mean ADOPT2 G-score\")\n",
    "plt.ylabel(\"Normalized histogram\")\n",
    "plt.show()\n",
    "\n",
    "scaled_hist_target_counts = np.round(hist * sigmoid).astype(int)\n",
    "print(f\"Original data size: {len(data)}\")\n",
    "print(f\"Target subsample size (sum of scaled_hist_target_counts): {np.sum(scaled_hist_target_counts)}\")\n",
    "\n",
    "subsampled_indices = []\n",
    "bin_assignment_for_data = np.digitize(data, bins_edges) - 1\n",
    "bin_assignment_for_data = np.clip(bin_assignment_for_data, 0, len(hist) - 1)\n",
    "\n",
    "\n",
    "original_indices = np.arange(len(data))\n",
    "for i in range(len(hist)):\n",
    "    target_count_for_this_bin = scaled_hist_target_counts[i]\n",
    "\n",
    "    if target_count_for_this_bin == 0:\n",
    "        continue\n",
    "    indices_of_data_in_bin_i = original_indices[bin_assignment_for_data == i]\n",
    "    num_available_in_bin = len(indices_of_data_in_bin_i)\n",
    "    num_to_sample_from_bin = min(target_count_for_this_bin, num_available_in_bin)\n",
    "\n",
    "    if num_to_sample_from_bin > 0:\n",
    "        chosen_indices = np.random.choice(\n",
    "            indices_of_data_in_bin_i,\n",
    "            size=num_to_sample_from_bin,\n",
    "            replace=False,\n",
    "        )\n",
    "        subsampled_indices.extend(chosen_indices)\n",
    "\n",
    "subsampled_indices = np.sort(subsampled_indices)  # just in case\n",
    "subsampled_data = data[subsampled_indices]\n",
    "subsampled_labels = np.array(current_labels)[subsampled_indices]\n",
    "assert (subsampled_data == gscores_adopt2_df[\"mean_gscore_adopt2\"].loc[subsampled_labels].to_numpy()).all()\n",
    "\n",
    "print(f\"Actual subsampled data size: {len(subsampled_data)}\")\n",
    "subsampled_hist, _ = np.histogram(subsampled_data, bins=bins_edges)\n",
    "\n",
    "plt.plot(bins_centers, hist, \"-+\", label=f\"original, tot={hist.sum():_}\")\n",
    "plt.plot(bins_centers, subsampled_hist, \"-x\", label=f\"subsampled, tot={int(subsampled_hist.sum()):_}\")\n",
    "plt.plot(bins_centers, scaled_hist, \"--\", label=f\"rescaled, tot={int(scaled_hist.sum()):_}\")\n",
    "# plt.xlim(0, 1)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Mean adopt2 zscore\")\n",
    "plt.ylabel(\"Histogram\")\n",
    "plt.show()\n",
    "\n",
    "mask = bins_centers > threshold\n",
    "print(\n",
    "    f\"tot={hist.sum():_}\"\n",
    "    f\", ordered={hist[mask].sum():_}, disordered={hist[~mask].sum():_}\"\n",
    "    f\", ratio={hist[mask].sum() / hist[~mask].sum():.2f}\",\n",
    ")\n",
    "print(\n",
    "    f\"tot={subsampled_hist.sum():_}\"\n",
    "    f\", ordered={subsampled_hist[mask].sum():_}, disordered={subsampled_hist[~mask].sum():_}\"\n",
    "    f\", ratio={subsampled_hist[mask].sum() / subsampled_hist[~mask].sum():.2f}\",\n",
    ")\n",
    "print(\n",
    "    f\"tot={scaled_hist.sum():_g}\"\n",
    "    f\", ordered={scaled_hist[mask].sum():_g}, disordered={scaled_hist[~mask].sum():_g}\"\n",
    "    f\", ratio={scaled_hist[mask].sum() / scaled_hist[~mask].sum():.2f}\",\n",
    ")\n",
    "\n",
    "subsampled_labels = sorted(subsampled_labels)\n",
    "subsampled_lengths = np.array([len(sequences[l]) for l in subsampled_labels])\n",
    "subsampled_mean_scores = gscores_adopt2_df[\"mean_gscore_adopt2\"].loc[subsampled_labels].to_numpy()\n",
    "\n",
    "print(f\"min length: {subsampled_lengths.min():_}, max length: {subsampled_lengths.max():_}\")\n",
    "plt.title(f\"Subsampled {len(subsampled_labels):_}\")\n",
    "plt.scatter(\n",
    "    subsampled_lengths,\n",
    "    subsampled_mean_scores,\n",
    "    c=subsampled_mean_scores,\n",
    "    s=1,\n",
    "    cmap=\"seismic\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "plt.colorbar(label=\"Mean ADOPT2 G-score\")\n",
    "plt.xlabel(\"sequence length\")\n",
    "plt.ylabel(\"Mean ADOPT2 G-score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
